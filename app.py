import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
import tempfile
import os
from openai import OpenAI
from typing import List, Tuple

# Streamlit page configuration
st.set_page_config(
    page_title="PDF “õ“±–∂–∞—Ç–ø–µ–Ω “õ–∞–∑–∞“õ—à–∞ —Å”©–π–ª–µ—Å—É",
    page_icon="üìö",
    layout="wide"
)

# Custom CSS for better UI
st.markdown("""
    <style>
    .stApp {
        max-width: 1200px;
        margin: 0 auto;
    }
    .stButton>button {
        width: 100%;
    }
    .error {
        color: red;
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        border: 1px solid #e2e8f0;
    }
    .user-message {
        background-color: #f8fafc;
    }
    .bot-message {
        background-color: #f0f9ff;
    }
    </style>
    """, unsafe_allow_html=True)

# Title and description
st.title("üìö PDF “õ“±–∂–∞—Ç–ø–µ–Ω “õ–∞–∑–∞“õ—à–∞ —Å”©–π–ª–µ—Å—É")
st.markdown("PDF “õ“±–∂–∞—Ç—Ç–∞—Ä—ã“£—ã–∑–±–µ–Ω “õ–∞–∑–∞“õ —Ç—ñ–ª—ñ–Ω–¥–µ —Å”©–π–ª–µ—Å—ñ“£—ñ–∑. –ö–µ–∑-–∫–µ–ª–≥–µ–Ω —Ç—ñ–ª–¥–µ–≥—ñ PDF “õ“±–∂–∞—Ç—Ç–∞—Ä“ì–∞ “õ–∞–∑–∞“õ—à–∞ —Å“±—Ä–∞“õ “õ–æ—è –∞–ª–∞—Å—ã–∑.")

# Initialize session state
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'suggested_questions' not in st.session_state:
    st.session_state.suggested_questions = []
if 'conversation' not in st.session_state:
    st.session_state.conversation = None

# API key input
api_key = st.text_input("OpenAI API –∫—ñ–ª—Ç—ñ–Ω –µ–Ω–≥—ñ–∑—ñ“£—ñ–∑:", type="password")

def validate_api_key(api_key: str) -> bool:
    """Validate OpenAI API key"""
    try:
        client = OpenAI(api_key=api_key)
        client.models.list()
        return True
    except Exception:
        return False

def process_pdf(uploaded_file):
    """Process PDF file and create conversation chain"""
    try:
        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        # Load and split the PDF
        loader = PyPDFLoader(tmp_file_path)
        documents = loader.load()
        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        documents = text_splitter.split_documents(documents)

        # Create embeddings and vector store
        embeddings = OpenAIEmbeddings(openai_api_key=api_key)
        vectorstore = FAISS.from_documents(documents, embeddings)

        # Create conversation chain
        llm = ChatOpenAI(
            temperature=0.7,
            model="gpt-4",
            openai_api_key=api_key
        )

        memory = ConversationBufferMemory(
            memory_key='chat_history',
            return_messages=True
        )

        conversation_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=vectorstore.as_retriever(search_kwargs={'k': 3}),
            memory=memory,
            return_source_documents=True,
            verbose=True
        )

        # Clean up temp file
        os.unlink(tmp_file_path)
        
        return conversation_chain
    except Exception as e:
        st.error(f"“ö“±–∂–∞—Ç—Ç—ã ”©“£–¥–µ—É –∫–µ–∑—ñ–Ω–¥–µ “õ–∞—Ç–µ –æ—Ä—ã–Ω –∞–ª–¥—ã: {str(e)}")
        return None

def generate_suggested_questions(context: str) -> List[str]:
    """Generate suggested questions based on context"""
    try:
        if not api_key:
            return []
        
        llm = ChatOpenAI(
            temperature=0.7, 
            openai_api_key=api_key,
            model="gpt-4"
        )
        
        prompt = f"""
        –ë–µ—Ä—ñ–ª–≥–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ–≥—ñ–∑—ñ–Ω–¥–µ 3 –º–∞“ì—ã–Ω–∞–ª—ã —Å“±—Ä–∞“õ “±—Å—ã–Ω—ã“£—ã–∑.
        –°“±—Ä–∞“õ—Ç–∞—Ä “õ–∞–∑–∞“õ —Ç—ñ–ª—ñ–Ω–¥–µ –±–æ–ª—É—ã –∫–µ—Ä–µ–∫.
        
        –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}
        """
        
        response = llm.predict(prompt)
        questions = [q.strip() for q in response.split('\n') if q.strip()]
        return questions[:3]
    except Exception:
        return []

# Validate API key when provided
if api_key:
    if not validate_api_key(api_key):
        st.error("–ñ–∞—Ä–∞–º—Å—ã–∑ API –∫—ñ–ª—Ç—ñ. ”®—Ç—ñ–Ω–µ–º—ñ–∑, —Ç–µ–∫—Å–µ—Ä—ñ–ø “õ–∞–π—Ç–∞ –µ–Ω–≥—ñ–∑—ñ“£—ñ–∑.")
        st.stop()

# File upload
uploaded_file = st.file_uploader("PDF “õ“±–∂–∞—Ç—Ç—ã –∂“Ø–∫—Ç–µ“£—ñ–∑", type="pdf")

if uploaded_file and api_key:
    if not st.session_state.conversation:
        with st.spinner("“ö“±–∂–∞—Ç ”©“£–¥–µ–ª—É–¥–µ..."):
            st.session_state.conversation = process_pdf(uploaded_file)
            if st.session_state.conversation:
                st.success("“ö“±–∂–∞—Ç —Å”ô—Ç—Ç—ñ –∂“Ø–∫—Ç–µ–ª–¥—ñ!")

# Chat interface
if st.session_state.conversation:
    # Chat input
    user_question = st.text_input("–°“±—Ä–∞“ì—ã“£—ã–∑–¥—ã “õ–∞–∑–∞“õ —Ç—ñ–ª—ñ–Ω–¥–µ –∂–∞–∑—ã“£—ã–∑:")
    
    # Suggested questions buttons
    if st.session_state.suggested_questions:
        st.write("–´“õ—Ç–∏–º–∞–ª —Å“±—Ä–∞“õ—Ç–∞—Ä:")
        cols = st.columns(len(st.session_state.suggested_questions))
        for i, question in enumerate(st.session_state.suggested_questions):
            if cols[i].button(question, key=f"suggested_q_{i}"):
                user_question = question

    if user_question:
        try:
            with st.spinner("–ñ–∞—É–∞–ø —ñ–∑–¥–µ—É–¥–µ..."):
                # Get response
                response = st.session_state.conversation({
                    "question": f"""
                    –°“±—Ä–∞“õ“õ–∞ “õ–∞–∑–∞“õ —Ç—ñ–ª—ñ–Ω–¥–µ –∂–∞—É–∞–ø –±–µ—Ä—ñ“£—ñ–∑.
                    –ï–≥–µ—Ä “õ“±–∂–∞—Ç—Ç–∞ –∂–∞—É–∞–ø —Ç–∞–±—ã–ª–º–∞—Å–∞, –æ–Ω—ã –∞–π—Ç—ã“£—ã–∑.
                    –°“±—Ä–∞“õ: {user_question}
                    """
                })
                
                # Store chat history
                st.session_state.chat_history.append((user_question, response['answer']))
                
                # Generate new suggested questions
                st.session_state.suggested_questions = generate_suggested_questions(response['answer'])

                # Refresh the page to show new content
                st.experimental_rerun()
                
        except Exception as e:
            st.error(f"–ñ–∞—É–∞–ø –∞–ª—É –∫–µ–∑—ñ–Ω–¥–µ “õ–∞—Ç–µ –æ—Ä—ã–Ω –∞–ª–¥—ã: {str(e)}")

# Display chat history
for i, (question, answer) in enumerate(st.session_state.chat_history):
    # User message
    st.markdown(f"""
    <div class="chat-message user-message">
        <b>üôã‚Äç‚ôÇÔ∏è –°“±—Ä–∞“õ:</b><br>{question}
    </div>
    """, unsafe_allow_html=True)
    
    # Bot message
    st.markdown(f"""
    <div class="chat-message bot-message">
        <b>ü§ñ –ñ–∞—É–∞–ø:</b><br>{answer}
    </div>
    """, unsafe_allow_html=True)

# Footer
st.markdown("""
---
–ë“±–ª –±–∞“ì–¥–∞—Ä–ª–∞–º–∞–Ω—ã –∂–∞—Å–∞“ì–∞–Ω –¢–∏–º—É—Ä –ë–µ–∫—Ç“±—Ä. 
–ï–≥–µ—Ä —Å—ñ–∑ –¥–µ –∂–∞—Å–∞–Ω–¥—ã –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∞—Ä“õ—ã–ª—ã –±–∞“ì–¥–∞—Ä–ª–∞–º–∞ –∂–∞—Å–∞—É–¥—ã “Ø–π—Ä–µ–Ω–≥—ñ“£—ñ–∑ –∫–µ–ª—Å–µ, 
”©–∑–≥–µ –¥–µ –∏–¥–µ—è–ª–∞—Ä –±–æ–π—ã–Ω—à–∞ –ñ–ò –∞—Ä“õ—ã–ª—ã ”©–∑—ñ“£—ñ–∑–≥–µ “õ–æ—Å—ã–º—à–∞ –∂–∞—Å–∞“ì—ã“£—ã–∑ –∫–µ–ª—Å–µ, 
–º–∞“ì–∞–Ω —Ö–∞–±–∞—Ä–ª–∞—Å—ã“£—ã–∑: instagram @timurbektur
""")
